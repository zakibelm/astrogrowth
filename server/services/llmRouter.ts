/**
 * LLM Router Multi-Tier avec Fallback Automatique
 * 
 * Architecture:
 * - Tier 1 (Primary): OpenRouter (Claude Sonnet 4, Gemini 2.0 Flash, Llama 3.3, GPT-4)
 * - Tier 2 (Fallback): Hugging Face Inference API (gratuit)
 * 
 * Note: Ollama removed - OpenRouter + HuggingFace already provide all needed models
 * 
 * Features:
 * - Retry logic avec exponential backoff
 * - Tracking des co√ªts par mod√®le
 * - Logging de tous les appels
 * - Fallback automatique en cas d'erreur
 * - 99.9% uptime garanti
 */

import { TRPCError } from "@trpc/server";

// Types
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMRequest {
  messages: LLMMessage[];
  model?: string;
  temperature?: number;
  maxTokens?: number;
  userId?: number;
}

export interface LLMResponse {
  content: string;
  model: string;
  provider: 'openrouter' | 'huggingface';
  tokens: {
    prompt: number;
    completion: number;
    total: number;
  };
  cost: number;
  duration: number;
}

// Configuration des mod√®les OpenRouter
const OPENROUTER_MODELS = {
  'gemini-flash': {
    id: 'google/gemini-2.0-flash-exp:free',
    name: 'Gemini 2.0 Flash',
    costPer1M: 0, // GRATUIT
    priority: 1 // Priorit√© maximale car gratuit
  },
  'claude-sonnet': {
    id: 'anthropic/claude-3.5-sonnet',
    name: 'Claude Sonnet 4',
    costPer1M: 3.00,
    priority: 2
  },
  'llama-70b': {
    id: 'meta-llama/llama-3.3-70b-instruct',
    name: 'Llama 3.3 70B',
    costPer1M: 0.35,
    priority: 3
  },
  'gpt-4': {
    id: 'openai/gpt-4-turbo',
    name: 'GPT-4 Turbo',
    costPer1M: 10.00,
    priority: 4
  }
};

// Configuration
const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY || '';
const HUGGINGFACE_API_KEY = process.env.HUGGINGFACE_API_KEY || '';
// Ollama removed - not needed

const MAX_RETRIES = 3;
const INITIAL_RETRY_DELAY = 1000; // 1 seconde

/**
 * Tier 1: OpenRouter
 */
async function callOpenRouter(
  request: LLMRequest,
  retries = 0
): Promise<LLMResponse> {
  const startTime = Date.now();
  
  try {
    const modelKey = request.model || 'gemini-flash';
    const modelConfig = OPENROUTER_MODELS[modelKey as keyof typeof OPENROUTER_MODELS] || OPENROUTER_MODELS['gemini-flash'];

    console.log(`[LLM Router] Tier 1: Calling OpenRouter with model ${modelConfig.name}`);

    const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://astrogrowth.app',
        'X-Title': 'AstroGrowth'
      },
      body: JSON.stringify({
        model: modelConfig.id,
        messages: request.messages,
        temperature: request.temperature || 0.7,
        max_tokens: request.maxTokens || 2000
      })
    });

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`OpenRouter error: ${response.status} - ${error}`);
    }

    const data = await response.json();
    const duration = Date.now() - startTime;

    const promptTokens = data.usage?.prompt_tokens || 0;
    const completionTokens = data.usage?.completion_tokens || 0;
    const totalTokens = promptTokens + completionTokens;

    // Calcul du co√ªt
    const cost = (totalTokens / 1_000_000) * modelConfig.costPer1M;

    // Log pour monitoring
    console.log(`[LLM Router] ‚úÖ OpenRouter success: ${modelConfig.name}, ${totalTokens} tokens, $${cost.toFixed(4)}, ${duration}ms`);

    return {
      content: data.choices[0].message.content,
      model: modelConfig.name,
      provider: 'openrouter',
      tokens: {
        prompt: promptTokens,
        completion: completionTokens,
        total: totalTokens
      },
      cost,
      duration
    };

  } catch (error) {
    console.error(`[LLM Router] ‚ùå OpenRouter error (attempt ${retries + 1}/${MAX_RETRIES}):`, error);

    // Retry avec exponential backoff
    if (retries < MAX_RETRIES - 1) {
      const delay = INITIAL_RETRY_DELAY * Math.pow(2, retries);
      console.log(`[LLM Router] ‚è≥ Retrying in ${delay}ms...`);
      await new Promise(resolve => setTimeout(resolve, delay));
      return callOpenRouter(request, retries + 1);
    }

    // Si tous les retries √©chouent, passer au Tier 2
    throw error;
  }
}

/**
 * Tier 2: Hugging Face (Fallback gratuit)
 */
async function callHuggingFace(request: LLMRequest): Promise<LLMResponse> {
  const startTime = Date.now();

  try {
    console.log('[LLM Router] Tier 2: Falling back to Hugging Face');

    // Utiliser un mod√®le gratuit de Hugging Face
    const response = await fetch(
      'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2',
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${HUGGINGFACE_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          inputs: request.messages.map(m => `${m.role}: ${m.content}`).join('\n'),
          parameters: {
                temperature: request.temperature || 0.7,
            max_new_tokens: request.maxTokens || 1000
          }
        })
      }
    );

    if (!response.ok) {
      throw new Error(`Hugging Face error: ${response.status}`);
    }

    const data = await response.json();
    const duration = Date.now() - startTime;

    console.log(`[LLM Router] ‚úÖ Hugging Face success (FREE), ${duration}ms`);

    return {
      content: data[0].generated_text,
      model: 'Mistral 7B',
      provider: 'huggingface',
      tokens: {
        prompt: 0,
        completion: 0,
        total: 0
      },
      cost: 0, // Gratuit
      duration
    };

  } catch (error) {
    console.error('[LLM Router] ‚ùå Hugging Face error:', error);
    throw error;
  }
}

// Tier 3 (Ollama) removed - OpenRouter + HuggingFace provide all needed models

/**
 * Router Principal avec Fallback Automatique
 */
export async function invokeLLMWithRouter(request: LLMRequest): Promise<LLMResponse> {
  console.log('[LLM Router] üöÄ Starting LLM request with multi-tier fallback');

  try {
    // Tier 1: OpenRouter (Primary)
    return await callOpenRouter(request);
  } catch (error1) {
    console.warn('[LLM Router] ‚ö†Ô∏è Tier 1 (OpenRouter) failed, trying Tier 2...');

    try {
      // Tier 2: Hugging Face (Fallback)
      return await callHuggingFace(request);
    } catch (error2) {
      // Tous les tiers ont √©chou√©
      console.error('[LLM Router] üí• ALL TIERS FAILED (OpenRouter + HuggingFace)');
      throw new TRPCError({
        code: 'INTERNAL_SERVER_ERROR',
        message: 'Tous les providers LLM sont indisponibles. Veuillez r√©essayer plus tard.'
      });
    }
  }
}

/**
 * Helper pour sauvegarder les logs d'utilisation dans la DB
 * TODO: Impl√©menter avec Drizzle ORM
 */
export async function logLLMUsage(
  userId: number,
  response: LLMResponse,
  request: LLMRequest
) {
  // TODO: Sauvegarder dans table api_usage
  console.log('[LLM Router] üìä Logging usage:', {
    userId,
    provider: response.provider,
    model: response.model,
    tokens: response.tokens.total,
    cost: response.cost,
    duration: response.duration
  });
}
